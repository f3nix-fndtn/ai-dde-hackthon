{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating Your Training Examples\n",
    "\n",
    "First, let’s create some examples that will help your assistant learn. We’ll use a friendly tool that makes this process easier. Create a new notebook called Legal_Assistant_Training and add this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "class ExampleCreator:\n",
    "    \"\"\"A friendly tool to help create training examples\"\"\"\n",
    "    def __init__(self):\n",
    "        self.examples = []\n",
    "        self.filename = 'training_data.jsonl'\n",
    "        # Create our input boxes\n",
    "        self.question_box = widgets.Textarea(\n",
    "            description='Question:',\n",
    "            placeholder='Type your legal question here...',\n",
    "            layout={'width': '90%', 'height': '100px'}\n",
    "        )\n",
    "        self.answer_box = widgets.Textarea(\n",
    "            description='Answer:',\n",
    "            placeholder='Type the correct answer here...',\n",
    "            layout={'width': '90%', 'height': '200px'}\n",
    "        )\n",
    "        # Create our buttons\n",
    "        self.save_button = widgets.Button(description='Save Example')\n",
    "        self.save_button.on_click(self.save_example)\n",
    "        self.display_button = widgets.Button(description='Show All Examples')\n",
    "        self.display_button.on_click(self.show_examples)\n",
    "        # Show our tool\n",
    "        display(self.question_box)\n",
    "        display(self.answer_box)\n",
    "        display(self.save_button)\n",
    "        display(self.display_button)\n",
    "    def save_example(self, button):\n",
    "        \"\"\"Save a new example\"\"\"\n",
    "        question = self.question_box.value.strip()\n",
    "        answer = self.answer_box.value.strip()\n",
    "        if not question or not answer:\n",
    "            print(\"❌ Please provide both a question and an answer!\")\n",
    "            return\n",
    "        # Create the example\n",
    "        example = {\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"human\", \"value\": question},\n",
    "                {\"from\": \"gpt\", \"value\": answer}\n",
    "            ]\n",
    "        }\n",
    "        # Save it\n",
    "        with open(self.filename, 'a') as f:\n",
    "            f.write(json.dumps(example) + '\\n')\n",
    "        self.examples.append(example)\n",
    "        # Clear the boxes for next example\n",
    "        self.question_box.value = ''\n",
    "        self.answer_box.value = ''\n",
    "        print(f\"✅ Example saved! You now have {len(self.examples)} examples.\")\n",
    "    def show_examples(self, button):\n",
    "        \"\"\"Show all saved examples\"\"\"\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Your {len(self.examples)} Training Examples:\\n\")\n",
    "        for i, example in enumerate(self.examples, 1):\n",
    "            print(f\"Example {i}:\")\n",
    "            print(f\"Q: {example['conversations'][0]['value']}\")\n",
    "            print(f\"A: {example['conversations'][1]['value']}\\n\")\n",
    "        # Show our tool again\n",
    "        display(self.question_box)\n",
    "        display(self.answer_box)\n",
    "        display(self.save_button)\n",
    "        display(self.display_button)\n",
    "# Create our example creator tool\n",
    "creator = ExampleCreator()\n",
    "print(\"✨ Example Creator is ready! Start adding your training examples above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s add some starter examples to help you understand the format. Add this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added starter examples to your training data!\n"
     ]
    }
   ],
   "source": [
    "# Some example legal questions and answers to get you started\n",
    "import json\n",
    "\n",
    "starter_examples = [\n",
    "    {\n",
    "        \"question\": \"What makes a contract valid?\",\n",
    "        \"answer\": \"A valid contract requires four essential elements: 1) Offer and acceptance, \"\n",
    "                  \"2) Consideration (something of value exchanged), 3) Intention to create \"\n",
    "                  \"legal relations, and 4) Capacity of the parties to contract. The agreement \"\n",
    "                  \"must also be legal and sufficiently certain in its terms.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does force majeure work in contracts?\",\n",
    "        \"answer\": \"Force majeure clauses excuse a party from performing their contractual \"\n",
    "                  \"obligations when extraordinary events beyond their control prevent \"\n",
    "                  \"performance. These events typically include natural disasters, wars, or \"\n",
    "                  \"government actions. The clause must specifically define what constitutes \"\n",
    "                  \"force majeure, and the party claiming it must prove the event's impact.\"\n",
    "    }\n",
    "]\n",
    "# Add these examples using our tool\n",
    "for example in starter_examples:\n",
    "    with open('training_data.jsonl', 'a') as f:\n",
    "        formatted_example = {\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"human\", \"value\": example[\"question\"]},\n",
    "                {\"from\": \"gpt\", \"value\": example[\"answer\"]}\n",
    "            ]\n",
    "        }\n",
    "        f.write(json.dumps(formatted_example) + '\\n')\n",
    "print(\"✅ Added starter examples to your training data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the tool to add your own examples. Here are some tips for creating good examples:\n",
    "\n",
    "    Include different types of questions:\n",
    "    “What is…” (definitions)\n",
    "    “How does…” (processes)\n",
    "    “Why is…” (reasoning)\n",
    "    “When should…” (timing)\n",
    "\n",
    "Make sure your answers:\n",
    "\n",
    "    Start with a clear main point\n",
    "    Include specific details\n",
    "    Use proper legal terminology\n",
    "    Stay concise but complete\n",
    "\n",
    "Try to create at least 50–70 examples before moving on to the next part. The more, the better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fine-Tuning and Deploying Your Model\n",
    "\n",
    "Once you have your training examples ready, you can use them to improve your model. This part is more technical and requires a new Colab notebook with more computational resources.\n",
    "\n",
    "Create a new notebook called Legal_Assistant_Finetuning and add these cells:\n",
    "Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -qqq \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --progress-bar off\n",
    "from torch import __version__\n",
    "from packaging.version import Version as V\n",
    "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
    "!pip install -qqq --no-deps {xformers} trl peft accelerate bitsandbytes triton --progress-bar off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import Libraries and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "# Load model\n",
    "max_seq_length = 2048\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    ")\n",
    "# Prepare model for PEFT\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
    "    use_rslora=True,\n",
    "    use_gradient_checkpointing=\"unsloth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare Data and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tokenizer with chat template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"}\n",
    ")\n",
    "def apply_template(examples):\n",
    "    messages = examples[\"conversations\"]\n",
    "    text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) \n",
    "            for message in messages]\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Load and prepare dataset\n",
    "dataset = load_dataset(\"json\", data_files='training_data.jsonl')\n",
    "dataset = dataset.map(apply_template, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset['train'],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=True,\n",
    "    args=TrainingArguments(\n",
    "        learning_rate=3e-4,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=1,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=10,\n",
    "        output_dir=\"output\",\n",
    "        seed=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Save and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for inference\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test with a sample query\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"What is your purpose?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "# Generate response\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs, \n",
    "    streamer=text_streamer, \n",
    "    max_new_tokens=128, \n",
    "    use_cache=True\n",
    ")\n",
    "# Save the model\n",
    "model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\n",
    "# Optional: Save GGUF versions\n",
    "model.save_pretrained_gguf(\"model\", tokenizer, \"q8_0\")\n",
    "quant_methods = [\"q2_k\", \"q3_k_m\", \"q4_k_m\", \"q5_k_m\", \"q6_k\", \"q8_0\"]\n",
    "for quant in quant_methods:\n",
    "    model.save_pretrained_gguf(f\"model_{quant}\", tokenizer, quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
