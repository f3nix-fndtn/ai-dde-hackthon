{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jO5aVH1HDKt9",
    "outputId": "6e6b305e-9adc-4131-e936-d76f0c8aed6c"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install markitdown langchain chromadb gradio spacy\n",
    "!pip install flash-attn git+https://github.com/huggingface/transformers.git triton\n",
    "\n",
    "\n",
    "! pip install pdfplumber pymupdf\n",
    "# Import basic libraries\n",
    "import os\n",
    "from markitdown import MarkItDown\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import logging\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# marksown coudl be improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "# Create documents folder\n",
    "DOCUMENTS_PATH = '/content/drive/MyDrive/legal_documents'\n",
    "if not os.path.exists(DOCUMENTS_PATH):\n",
    "    os.makedirs(DOCUMENTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAAi0qC3D_dR"
   },
   "source": [
    "Cell 2: Document Processor Class\n",
    "\n",
    "This cell defines our core document processing system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAvH6rSuG6Zv"
   },
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "class PDFProccessor:\n",
    "\n",
    "  pdf_path = None\n",
    "\n",
    "  def __init__(self, pdf_path):\n",
    "    self.pdf_path = pdf_path\n",
    "    pass\n",
    "\n",
    "  def process_pdf(self):\n",
    "    text = self.open_pdf()\n",
    "    print(self.remove_page_numbers(text))\n",
    "\n",
    "  def open_pdf(self):\n",
    "    with pdfplumber.open(self.pdf_path) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "  def remove_page_numbers(self, text):\n",
    "    extrapolations_pattern =  r\"^\\s*THE\\s+EXTRAPOLATIONS\\s+DOCUMENT(?:\\s*\\n\\s*|\\s+)*\\d+\\s*$\"\n",
    "    dangling_page_number = r\"^\\s*\\d+\\s*$\"\n",
    "    clean_text = re.sub(extrapolations_pattern, \"\", text, flags=re.MULTILINE)\n",
    "    return re.sub(dangling_page_number, \"\", clean_text, flags=re.MULTILINE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pPRn0pmzNm_"
   },
   "source": [
    "This I think might help reduce the tokens for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-epEPaNMyweF",
    "outputId": "64c9506b-33dc-48a2-ce2f-05194248c67c"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pdfplumber\n",
    "\n",
    "def extract_chunks_after_toc(pdf_path, start_after=\"I. Introduction\"):\n",
    "    content_to_process = \"\"\n",
    "    start_processing = False\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "\n",
    "            if not start_processing:\n",
    "                # Detect and skip the Table of Contents\n",
    "                if \"Table of Contents\" in text:\n",
    "                    continue\n",
    "\n",
    "                # Look for the start point after TOC\n",
    "                match = re.search(rf\"{re.escape(start_after)}\", text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    # Start processing text after the specified section\n",
    "                    _, remaining_text = re.split(rf\"{re.escape(start_after)}\", text, flags=re.IGNORECASE, maxsplit=1)\n",
    "                    content_to_process += remaining_text.strip()\n",
    "                    start_processing = True\n",
    "            else:\n",
    "                # Append remaining text from the document\n",
    "                content_to_process += \"\\n\" + text.strip()\n",
    "\n",
    "    # Chunk content further (if needed)\n",
    "    split_pattern = r\"(?=\\n[A-Z]\\.\\s)|(?=\\n[A-Z]\\.\\d+\\s)\"  # Match sections A., A.1, etc.\n",
    "    chunks = re.split(split_pattern, content_to_process)\n",
    "    cleaned_chunks = [chunk.strip() for chunk in chunks if chunk.strip()]  # Remove empty chunks\n",
    "\n",
    "    return cleaned_chunks\n",
    "\n",
    "\n",
    "# # Example Usage\n",
    "# pdf_path = \"/content/drive/MyDrive/legal_documents/Leiden Guidelines on the Use of DDE in ICCTs_20220404.pdf\"\n",
    "# chunks = extract_chunks_after_section(pdf_path)\n",
    "# for i, chunk in enumerate(chunks):\n",
    "#     print(f\"Chunk \\n{i+1}:\\n{chunk}\\n{'-'*50}\")\n",
    "\n",
    "\n",
    "chunks = extract_chunks_after_toc(pdf_path)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i + 1}:\\n{chunk}\\n{'-' * 50}\")\n",
    "\n",
    "\n",
    "def extract_headers_and_chunks(chunks):\n",
    "    headers_with_chunks = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # Match header at the start of the chunk (e.g., C. Aerial and Satellite Images)\n",
    "        match = re.match(r\"^[A-Z](?:\\.\\d+)?\\.\\s.+\", chunk)\n",
    "        if match:\n",
    "            header = match.group(0).strip()  # Extract the header\n",
    "            content = chunk[len(header):].strip()  # Remaining content after the header\n",
    "            headers_with_chunks.append({\"header\": header, \"content\": content})\n",
    "        else:\n",
    "            headers_with_chunks.append({\"header\": \"Unknown Header\", \"content\": chunk.strip()})\n",
    "\n",
    "    return headers_with_chunks\n",
    "\n",
    "# Example usage with chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7qboTjRY8rCS",
    "outputId": "fc308c05-c41d-4bf4-d685-0982c3a5b5fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section Header: Unknown Header\n",
      "\n",
      "Section Header: A. Definition of DDE\n",
      "\n",
      "Section Header: B. Methodology\n",
      "\n",
      "Section Header: C. Structure of the Leiden Guidelines\n",
      "\n",
      "Section Header: D. Scope of the Leiden Guidelines\n",
      "\n",
      "Section Header: A. Videos\n",
      "\n",
      "Section Header: B. Photographs\n",
      "\n",
      "Section Header: C. Aerial and Satellite Images\n",
      "\n",
      "Section Header: D. Intercepts\n",
      "\n",
      "Section Header: E. Call Data Records\n",
      "\n",
      "Section Header: F. Audio Recordings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "headers_with_chunks = extract_headers_and_chunks(chunks)\n",
    "\n",
    "for item in headers_with_chunks:\n",
    "    print(f\"Section Header: {item['header']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v4ZDnQNeDWuV",
    "outputId": "736dd8b4-d168-4144-c83d-a84e6ce47984"
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# vecorisation could be improved\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the document processor with necessary components.\"\"\"\n",
    "        # Set up embedding model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.model.eval()\n",
    "        # Initialize document converter\n",
    "        self.md = MarkItDown()\n",
    "        # Set up vector database\n",
    "        self.vector_db = chromadb.Client()\n",
    "        self.collection = self.vector_db.get_or_create_collection(name=\"legal_docs\")\n",
    "\n",
    "    def process_document(self, file_path):\n",
    "        \"\"\"Convert document to text and generate embeddings.\"\"\"\n",
    "        try:\n",
    "            pdf_processor = PDFProccessor(file_path)\n",
    "            # Convert document to text\n",
    "            conversion_result = self.md.convert(file_path)\n",
    "            conversion_result_text = self.md.convert(file_path).text_content\n",
    "            # TODO - teh leiden guidelines contain a section of keywwords for each section - these should be parsed out and each section should be stored seperately\n",
    "            conversion_result_text = pdf_processor.remove_page_numbers(conversion_result_text)\n",
    "            print(conversion_result)\n",
    "\n",
    "            # Create embeddings\n",
    "            inputs = self.tokenizer(\n",
    "                conversion_result_text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True\n",
    "            )\n",
    "            # Use GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                self.model.to('cuda')\n",
    "                inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "            # Generate embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy().tolist()\n",
    "            # **************\n",
    "            return {\n",
    "                'text': conversion_result_text,\n",
    "                'embeddings': embeddings,\n",
    "                'metadata': {}\n",
    "            }\n",
    "            # Note: we don't seem to get metadata from the docs anyway so bettter manually adding\n",
    "            getattr(conversion_result, 'metadata', {})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing document {file_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def store_document(self, doc_id, text, embedding, metadata=None):\n",
    "        \"\"\"Store document in the vector database.\"\"\"\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "        self.collection.add(\n",
    "            documents=[text],\n",
    "            embeddings=[embedding],\n",
    "            metadatas=[metadata],\n",
    "            ids=[doc_id]\n",
    "        )\n",
    "\n",
    "    def find_relevant_documents(self, query, metadata_titles, metadata_keywords, n_results=3):\n",
    "        \"\"\"Find relevant documents for a given query.\"\"\"\n",
    "\n",
    "        filter_dict = {}\n",
    "\n",
    "        if metadata_titles and len(metadata_titles) > 0:\n",
    "          filter_dict[\"title\"] = {\"$in\": metadata_titles}\n",
    "\n",
    "        if metadata_keywords:\n",
    "          filter_dict[\"keywords\"] = {\"$in\": metadata_keywords}\n",
    "\n",
    "        filter = None if len(filter_dict) == 0 else filter_dict\n",
    "\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query],\n",
    "            where=filter,\n",
    "            n_results=n_results\n",
    "        )\n",
    "        return [\n",
    "            {\n",
    "                'text': doc_text,\n",
    "                'id': results['ids'][0][i],\n",
    "                'metadata': results['metadatas'][0][i]\n",
    "            }\n",
    "            for i, doc_text in enumerate(results['documents'][0])\n",
    "        ]\n",
    "# Initialize processor\n",
    "processor = DocumentProcessor()\n",
    "# Move to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    processor.model = processor.model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qik6NtjUELyB"
   },
   "source": [
    "Cell 3: Set Up Google Drive Integration or local folder\n",
    "\n",
    "This cell connects to your Google Drive to access documents. Create a folder on your Drive called “legal_documents”. Or alter the code bellow accordingly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkS0b_2JFRG8"
   },
   "source": [
    "Cell 4: Set Up LLaMA Model\n",
    "\n",
    "Go to HuggingFace and search for the llama model you want to use. For example, 3.1. Request permission to use it and get a HuggingFace token.\n",
    "\n",
    "This cell initializes the LLaMA model for generating responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "5dd4e6f7b4284c64bcc751cc4f5aaf32",
      "4a9e7bd7d606425b92e3e06c64363c15",
      "d235a890782442f0a81f295f656b2117",
      "f71344e37b7746f38871fef74cbcc3c7",
      "6f45434fab7040b990777e68317d27e3",
      "408cdf9700364b3faae550215e9bafb0",
      "390cfd3cee2a47a99f778bde80de17b7",
      "fc7084c581db4380a9de2a2c78432728",
      "9040c75315d748ddba420442f6d4786b",
      "10c9476828b341678bb1189df88cb693",
      "89290c0c05aa4192b5726d1f61ca8ba4"
     ]
    },
    "id": "1wu9Rc0_FVt7",
    "outputId": "cde28016-b6f3-46c0-9346-099db53661e3"
   },
   "outputs": [],
   "source": [
    "\n",
    "HF_TOKEN = \"your-token\"\n",
    "\n",
    "# fine tune llama to make thsi better\n",
    "# experiment with prompts\n",
    "# open ai option\n",
    "#\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B\",\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B\",\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaiYpiaDFhLW"
   },
   "source": [
    "Cell 5: Process Documents\n",
    "\n",
    "This cell processes all documents in your legal_documents folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SztXztcd7VrS",
    "outputId": "6eb0a54f-5e56-40a3-ad71-25e98d8b6ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Leiden Guidelines on the Use of Digitally Derived Evidence in International Criminal Courts and Tribunals', 'author': 'Unknown', 'category': '', 'keywords': ''}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metadata_list = {\n",
    "    'doc_0_Leiden Guidelines on the Use of DDE in ICCTs_20220404.pdf' : {\n",
    "      'title': 'Leiden Guidelines on the Use of Digitally Derived Evidence in International Criminal Courts and Tribunals',\n",
    "      'author': 'Unknown',\n",
    "      'section-header': '',\n",
    "      'category': '',\n",
    "      'keywords': '',\n",
    "    },\n",
    "    'doc_1_Extrapolations from Case Law on DDE in ICCTs_20220405.pdf': {\n",
    "      'title': 'Extrapolations from case law on the use of digitally derived evidence (dde) before international criminal courts and tribunals',\n",
    "      'author': 'Unknown',\n",
    "      'section-header': '',\n",
    "      'category': '',\n",
    "      'keywords': '',\n",
    "      },\n",
    "    'doc_2_Case Summaries-The Use of DDE before ICCTs.pdf': {\n",
    "      'title': 'Analysis of Digitally Derived Evidence from the Jurisprudence of International Tribunals: Cases from the ICC, ICTR, ICTY, IRMCT, SCSL and STL',\n",
    "      'author': 'Unknown',\n",
    "      'section-header': '',\n",
    "      'category': '',\n",
    "      'keywords': '',\n",
    "      },\n",
    "    'doc_3_Fact-Finding-Missions.pdf': {\n",
    "      'title': 'REPORT ON DIGITALLY DERIVED EVIDENCE USED IN UN HUMAN RIGHTS FACT-FINDING MISSIONS APPROACHES AND STANDARDS OF PROOF',\n",
    "      'author': 'Unknown',\n",
    "      'section-header': '',\n",
    "      'category': '',\n",
    "      'keywords': '',\n",
    "\n",
    "      },\n",
    "    'doc_4_DDE in ICL.pdf': {\n",
    "      'title': 'REPORT ON DIGITALLY DERIVED EVIDENCE IN INTERNATIONAL CRIMINAL LAW',\n",
    "      'author': 'Unknown',\n",
    "      'section-header': '',\n",
    "      'category': '',\n",
    "      'keywords': '',\n",
    "      },\n",
    "}\n",
    "# TODO 'section-header': '' each PDF needs chunked and the section header of each chunk need added to the metadata\n",
    "print(metadata_list['doc_0_Leiden Guidelines on the Use of DDE in ICCTs_20220404.pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wv22Av64FylR"
   },
   "outputs": [],
   "source": [
    "# Get list of documents\n",
    "# improves the inteface\n",
    "# disclaimers - agent answers - next agent makes it accesible - next agent is a lawyer that critiques answer\n",
    "\n",
    "document_files = [\n",
    "    f for f in os.listdir(DOCUMENTS_PATH)\n",
    "    if f.endswith(('.pdf', '.docx', '.txt', '.html', '.pptx'))\n",
    "]\n",
    "if not document_files:\n",
    "    print(\"⚠️ No documents found! Add some to your legal_documents folder\")\n",
    "else:\n",
    "    print(f\"Found {len(document_files)} documents to process\")\n",
    "    for idx, document in enumerate(document_files):\n",
    "        print(f\"Processing {document}...\")\n",
    "        file_path = os.path.join(DOCUMENTS_PATH, document)\n",
    "        # Process document\n",
    "        result = processor.process_document(file_path)\n",
    "        # Store in database\n",
    "        doc_id = f\"doc_{idx}_{document}\"\n",
    "        metadata = metadata_list[doc_id]\n",
    "        processor.store_document(\n",
    "            doc_id=doc_id,\n",
    "            text=result['text'],\n",
    "            embedding=result['embeddings'],\n",
    "            metadata=metadata\n",
    "        )\n",
    "        print(f\"✅ Finished storing {document} in Chroma\\n\")\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error processing {document}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6AC3HnbF5iu"
   },
   "source": [
    "Cell 6: Question-Answering Function\n",
    "\n",
    "This cell defines the function that generates answers using LLaMA. You may alter the values if you know what you’re doing :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwwDNz-9BQLV"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Sample function for extracting keywords\n",
    "def extract_keywords(text):\n",
    "    doc = nlp(text)\n",
    "    keywords = [token.text for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wA9qQ0VPgmYD",
    "outputId": "dab9915d-4ae3-4314-bd16-3628e11951d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:86: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/fuzzywuzzy-0.18.0.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
      "  for line in open(toplevel):\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ivz4QTEYF4EX",
    "outputId": "c108388d-04a4-4a11-f7f6-adae53040e5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:914: ImportWarning: _PyDrive2ImportHook.find_spec() not found; falling back to find_module()\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _GenerativeAIImportHook.find_spec() not found; falling back to find_module()\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I am using the Leiden Guidelines and case law in my work as a human rights lawyer. The Leiden Guidelines provide guidance on how to use digitally derived evidence (DDE) in international criminal courts and tribunals, while case law provides examples of how DDE has been used in previous cases. By following these guidelines and learning from past experiences, I can ensure that my use of DDE is lawful and effective in achieving justice for victims of human rights violations.\n",
      "\n",
      " DOCUMENTS: Leiden Guidelines on the Use of Digitally Derived Evidence in International Criminal Courts and Tribunals,\n",
      " - Extrapolations from case law on the use of digitally derived evidence (dde) before international criminal courts and tribunals\n",
      "\n",
      " NOTE: this is only guidance based on past case law.\n"
     ]
    }
   ],
   "source": [
    "# --- Document Excerpts ---\n",
    "# {truncated_context}\n",
    "# Function to enhance the query by adding the extracted keywords\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "titles_list = [doc_metadata['title'] for doc_metadata in metadata_list.values()]\n",
    "\n",
    "def find_full_title(query, titles_list, threshold=60):\n",
    "    matches = [title for title in titles_list if fuzz.partial_ratio(query.lower(), title.lower()) >= threshold]\n",
    "    return matches\n",
    "\n",
    "keywords_list = [doc_metadata['keywords'] for doc_metadata in metadata_list.values()]\n",
    "\n",
    "def find_keywords(search_terms, keywords):\n",
    "    for search_term in search_terms:\n",
    "        if search_term.lower() in keywords:\n",
    "            return search_term.lower()\n",
    "    return None  # No matching title found\n",
    "\n",
    "# TODO - if the chat bot doesn't recognise any keywords it should prompt back and say something like:\n",
    "#  'Ask me about digital evidence related to photographic, video or etc. evidence.'\n",
    "def ask_question_llama(question, leiden_guide_lines, case_law):\n",
    "    # always Leiden Guidelines\n",
    "    metadata_titles = [titles_list[0]]\n",
    "    if case_law:\n",
    "      # Extrapolations from case law\n",
    "      metadata_titles.append(titles_list[1])\n",
    "    # if\n",
    "      # Cases from the ICC, ICTR, ICTY, IRMCT, SCSL and STL\n",
    "    # if\n",
    "      # UN HUMAN RIGHTS FACT-FINDING\n",
    "    # if\n",
    "      # INTERNATIONAL CRIMINAL LAW\n",
    "    extracted_keywords = extract_keywords(question)\n",
    "\n",
    "    metadata_keywords = find_keywords(extracted_keywords, keywords_list)\n",
    "    # metadata_titles = find_full_title(question, titles_list)\n",
    "    \"\"\"Generate an answer to a legal question using LLaMA.\"\"\"\n",
    "    # Get relevant documents\n",
    "    relevant_docs = processor.find_relevant_documents(query=question, metadata_keywords=metadata_keywords, metadata_titles=metadata_titles, n_results=5)\n",
    "\n",
    "    # Prepare context\n",
    "    context_pieces = [doc['text'][4000:5000] for doc in relevant_docs]\n",
    "    titles = [doc['metadata']['title'] for doc in relevant_docs]\n",
    "    keywords = [doc['metadata']['keywords'] for doc in relevant_docs]\n",
    "    truncated_context = \"\\n\".join(context_pieces)\n",
    "    # Create prompt\n",
    "\n",
    "    full_prompt = f\"\"\"You are a Human Rights Lawyer using the documents below to answer the following question.\n",
    "--- Document Titles ---\n",
    "{titles}\n",
    "--- Document Keywords ---\n",
    "{keywords}\n",
    "--- Question ---\n",
    "{question}\n",
    "\n",
    "Based on the documents above, provide a clear, concise answer. If relevant, refer to legal precedent, case law, or any specific details from the documents. Do not simply restate the question; make sure the answer is grounded in the provided content.\n",
    "\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    # Prepare for generation\n",
    "    if torch.cuda.is_available():\n",
    "        model.to('cuda')\n",
    "    # TODO - increase the token limit to allow for more text\n",
    "    inputs = tokenizer(\n",
    "        full_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=1024,\n",
    "        truncation=True\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "    # TODO - increase the token limit to allow for more text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=800,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "    # Process output\n",
    "    raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"Answer:\" in raw_output:\n",
    "        final_answer = raw_output.split(\"Answer:\", 1)[1].strip()\n",
    "    else:\n",
    "        final_answer = raw_output\n",
    "\n",
    "    docs = \",\\n - \".join(metadata_titles)\n",
    "    final_answer = final_answer + f'\\n\\n DOCUMENTS: {docs}' + '\\n\\n NOTE: this is only guidance based on past case law.'\n",
    "    # print(final_answer )\n",
    "    return final_answer\n",
    "\n",
    "    #\n",
    "\n",
    "answer = ask_question_llama(\"Are you using the Leiden Guidelines and case law ?\", True, True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bBSaMH0GCLf"
   },
   "source": [
    "Cell 7: Create User Interface\n",
    "\n",
    "Finally, we can also create the Gradio interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 726
    },
    "id": "tkXmnZ3qGGIT",
    "outputId": "25b33af4-f969-47c9-8068-56b9993906d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gradio/http_server.py:120: ResourceWarning: unclosed <socket.socket fd=275, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('0.0.0.0', 0)>\n",
      "  s = socket.socket()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/local/lib/python3.10/dist-packages/gradio/http_server.py:120: ResourceWarning: unclosed <socket.socket fd=277, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('0.0.0.0', 0)>\n",
      "  s = socket.socket()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://e77096fa43f1b3f0da.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e77096fa43f1b3f0da.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def create_interface():\n",
    "    demo = gr.Interface(\n",
    "        fn=ask_question_llama,\n",
    "        inputs=[\n",
    "            gr.Textbox(\n",
    "                label=\"Your Legal Question\",\n",
    "                placeholder=\"Ask any question about your legal documents...\",\n",
    "                lines=3\n",
    "            ),\n",
    "            gr.Checkbox(label=\"Open Source Invesitgator\", value=True),\n",
    "            gr.Checkbox(label=\"Lawyer\", value=False),\n",
    "            ],\n",
    "        outputs=[\n",
    "            gr.Markdown(\n",
    "                label=\"Answer\",\n",
    "            )\n",
    "        ],\n",
    "\n",
    "        title=\"Legal Document Assistant\",\n",
    "        description=\"This AI assistant can answer questions about your legal documents.\"\n",
    "    )\n",
    "    return demo\n",
    "# Launch interface\n",
    "demo = create_interface()\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFcYMrFmFiXs"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "Everything below here from the original Medium Article\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ft2K0x1bEH87"
   },
   "source": [
    "Option A: Basic Processing (Free, No API Key Needed)\n",
    "\n",
    "If you want to use the basic document processor (good for text-based documents), create a new cell and run this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lO6oS5sZEGZG"
   },
   "outputs": [],
   "source": [
    "# # Set up basic document processing\n",
    "# processor = DocumentProcessor()\n",
    "# # Add basic document conversion\n",
    "# processor.md = MarkItDown()\n",
    "# logger.info(\"✅ Basic document processor ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6fyQZUjEV7c"
   },
   "source": [
    "Option B: Enhanced Processing (Requires OpenAI API Key)\n",
    "\n",
    "If you want enhanced processing with better image handling and understanding, create a new cell and run this code instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRqKsdM2ENmw"
   },
   "outputs": [],
   "source": [
    "# # First, set your OpenAI key\n",
    "# OPENAI_API_KEY = \"your-openai-key-here\"  # Replace with your actual key\n",
    "\n",
    "# # Set up enhanced document processing\n",
    "# from openai import OpenAI\n",
    "# processor = DocumentProcessor()\n",
    "# client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "# processor.md = MarkItDown(llm_client=client, llm_model=\"gpt-4\")\n",
    "# logger.info(\"✅ Enhanced document processor ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIlFcdrCHnyl"
   },
   "source": [
    "Part 3: Choose Your Storage System\n",
    "\n",
    "Next, decide how you want to store your processed documents. You have two options:\n",
    "Option A: Local Storage (Free, Good for Small Projects)\n",
    "\n",
    "If you want to use local storage, create a new cell and run this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMVQoxjBHwRy"
   },
   "outputs": [],
   "source": [
    "# Set up local storage with Chroma\n",
    "import chromadb\n",
    "processor.vector_db = chromadb.Client()\n",
    "logger.info(\"✅ Local storage ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4-j9Az7H2XX"
   },
   "source": [
    "Option B: Cloud Storage (Requires Qdrant Account)\n",
    "\n",
    "If you want cloud storage, you need to create an account with Qdrant. Follow their instructions. Then, create a new cell and run this code instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5JB5BRVNH7L1"
   },
   "outputs": [],
   "source": [
    "# First, set your Qdrant credentials\n",
    "QDRANT_URL = \"your-qdrant-url-here\"      # Replace with your URL\n",
    "QDRANT_API_KEY = \"your-qdrant-key-here\"  # Replace with your key\n",
    "# Set up cloud storage with Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "processor.vector_db = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "logger.info(\"✅ Cloud storage ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNoi8cj_IBUY"
   },
   "source": [
    "Part 4: Connect to Your Documents\n",
    "\n",
    "Now you’ll connect to your documents. Everyone should use Google Drive for this part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZo1c1A8IHVz",
    "outputId": "575da8fe-2cb5-4306-dc57-8a66177b502c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "# Mount Google Drive\n",
    "logger.info(\"Connecting to Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "# Set up documents folder\n",
    "DOCUMENTS_PATH = '/content/drive/MyDrive/legal_documents'\n",
    "if not os.path.exists(DOCUMENTS_PATH):\n",
    "    os.makedirs(DOCUMENTS_PATH)\n",
    "    logger.info(\"Created 'legal_documents' folder in your Google Drive\")\n",
    "    logger.info(\"⚠️ Please add your documents to this folder before continuing\")\n",
    "else:\n",
    "    logger.info(\"✅ Found 'legal_documents' folder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeeYWEHEHLYJ"
   },
   "source": [
    "Part 5: Choose Your AI Model\n",
    "\n",
    "Finally, decide which AI model you want to use for answering questions. You have two options:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DA_9wQFFG6ZT"
   },
   "source": [
    "Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-9GR0AbG-eL"
   },
   "outputs": [],
   "source": [
    "# First, ensure you have your OpenAI key\n",
    "OPENAI_API_KEY = \"your-openai-key-here\"  # Replace with your actual key\n",
    "# Set up GPT-4\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    temperature=0.7\n",
    ")\n",
    "logger.info(\"✅ GPT-4 model ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhggK9BzF7ZP"
   },
   "source": [
    "Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87iMolWOF8w7"
   },
   "outputs": [],
   "source": [
    "# First, set your HuggingFace token\n",
    "HF_TOKEN = \"your-huggingface-token-here\"  # Replace with your actual token\n",
    "# Set up Llama\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B\",\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B\",\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "logger.info(\"✅ Llama model ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNHgD5RjIioe"
   },
   "source": [
    "Part 6: Process Your Documents\n",
    "\n",
    "Now that everything is set up, create a new cell to process your documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xA29aqVPIjQJ"
   },
   "outputs": [],
   "source": [
    "# Get list of documents\n",
    "document_files = [f for f in os.listdir(DOCUMENTS_PATH)\n",
    "                 if f.endswith(('.pdf', '.docx', '.txt', '.html', '.pptx'))]\n",
    "\n",
    "if not document_files:\n",
    "    logger.info(\"⚠️ No documents found! Add some to your legal_documents folder\")\n",
    "else:\n",
    "    logger.info(f\"Found {len(document_files)} documents to process\")\n",
    "    # Process each document\n",
    "    for document in document_files:\n",
    "        try:\n",
    "            logger.info(f\"Processing {document}...\")\n",
    "            file_path = os.path.join(DOCUMENTS_PATH, document)\n",
    "            result = processor.process_document(file_path)\n",
    "            logger.info(f\"✅ Processed {document}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {document}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72YXoZ9cIvaX"
   },
   "source": [
    "Part 7: Final Testing\n",
    "\n",
    "Create one last cell to test your assistant. The code depends on which AI model you chose:\n",
    "If You Chose GPT-4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_NeWbxmIv6y"
   },
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    prompt = f\"Based on the legal documents, please answer: {question}\"\n",
    "    return model.predict(prompt)\n",
    "\n",
    "# Test the system\n",
    "test_question = \"What are the main terms of the agreement?\"\n",
    "answer = ask_question(test_question)\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-d5pFccI-Jr"
   },
   "source": [
    "If You Chose Llama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ioXpuGaVI-mb"
   },
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    prompt = f\"Question about legal documents: {question}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.7)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the system\n",
    "test_question = \"What are the main terms of the agreement?\"\n",
    "answer = ask_question(test_question)\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmIye0rVJJIM"
   },
   "source": [
    "Part 8: Creating Your User Interface\n",
    "\n",
    "Now that we have our assistant working, let’s create a user-friendly interface. This will make it easy for anyone to ask questions about your legal documents. We’ll use Gradio, a tool that helps create web interfaces for AI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "N2rWAa5XJJln",
    "outputId": "c80392bb-d2a5-44c5-9b01-a9a91c6ad821"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-5.12.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.6)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.5.4 (from gradio)\n",
      "  Downloading gradio_client-1.5.4-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.5)\n",
      "Collecting markupsafe~=2.0 (from gradio)\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.13)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.4)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.9.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.3)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.34.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.4->gradio) (2024.10.0)\n",
      "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.4->gradio) (14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Downloading gradio-5.12.0-py3-none-any.whl (57.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.5.4-py3-none-any.whl (321 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.4/321.4 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.9.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Installing collected packages: tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, safehttpx, gradio-client, gradio\n",
      "  Attempting uninstall: markupsafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "Successfully installed aiofiles-23.2.1 ffmpy-0.5.0 gradio-5.12.0 gradio-client-1.5.4 markupsafe-2.1.5 python-multipart-0.0.20 ruff-0.9.1 safehttpx-0.1.6 semantic-version-2.10.0 tomlkit-0.13.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:86: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/MarkupSafe-2.1.5.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
      "  for line in open(toplevel):\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:86: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/semantic_version-2.10.0.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
      "  for line in open(toplevel):\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "986396d9685c4f939bdb45466df4ba78",
       "pip_warning": {
        "packages": [
         "markupsafe"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gradio installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install Gradio for our interface\n",
    "!pip install gradio\n",
    "print(\"✅ Gradio installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vynfB7t5J20u"
   },
   "source": [
    "Step 2: Choose Your Interface Setup\n",
    "\n",
    "Now you’ll need to set up the interface based on which AI model you chose earlier. Pick the option that matches your previous choice:\n",
    "Option A: Interface with GPT-4\n",
    "\n",
    "If you’re using GPT-4, create a new cell and run this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "BZkCszVqJNOe",
    "outputId": "1914213c-73d6-4bb8-d118-63bc4193f4a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/websockets/legacy/__init__.py:6: DeprecationWarning: websockets.legacy is deprecated; see https://websockets.readthedocs.io/en/stable/howto/upgrade.html for upgrade instructions\n",
      "  warnings.warn(  # deprecated in 14.0 - 2024-11-09\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _PyDrive2ImportHook.find_spec() not found; falling back to find_module()\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _GenerativeAIImportHook.find_spec() not found; falling back to find_module()\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n",
      "<frozen importlib._bootstrap>:914: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7b310c25edac>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdemo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Create and launch the interface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mdemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_gpt4_interface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# share=True creates a public link you can share\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ Interface is ready! Click the link above to start using your assistant.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "def create_gpt4_interface(model, processor):\n",
    "    \"\"\"Creates an interface for GPT-4 based assistant\"\"\"\n",
    "    def get_response(question: str) -> str:\n",
    "        \"\"\"Process a question and get GPT-4's response\"\"\"\n",
    "        try:\n",
    "            # Find relevant document sections\n",
    "            relevant_docs = processor.find_relevant_documents(question)\n",
    "            context = \"\\n\\n\".join(doc.text for doc in relevant_docs)\n",
    "            # Create our prompt\n",
    "            prompt = f\"\"\"As a legal expert, please answer this question based on\n",
    "            the provided documents:\n",
    "            Documents: {context}\n",
    "            Question: {question}\n",
    "            \"\"\"\n",
    "            # Get GPT-4's response\n",
    "            response = model.predict(prompt)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing question: {str(e)}\")\n",
    "            return f\"I encountered an error: {str(e)}\"\n",
    "    # Create the interface\n",
    "    demo = gr.Interface(\n",
    "        fn=get_response,\n",
    "        inputs=[\n",
    "            gr.Textbox(\n",
    "                label=\"Your Legal Question\",\n",
    "                placeholder=\"Ask any question about your legal documents...\",\n",
    "                lines=3\n",
    "            )\n",
    "        ],\n",
    "        outputs=[\n",
    "            gr.Textbox(\n",
    "                label=\"Answer\",\n",
    "                lines=10\n",
    "            )\n",
    "        ],\n",
    "        title=\"Legal Document Assistant (GPT-4)\",\n",
    "        description=\"\"\"This AI assistant can answer questions about your legal documents.\n",
    "        It uses GPT-4 to provide accurate, contextual responses based on your documents.\"\"\"\n",
    "    )\n",
    "    return demo\n",
    "# Create and launch the interface\n",
    "demo = create_gpt4_interface(model, processor)\n",
    "demo.launch(share=True)  # share=True creates a public link you can share\n",
    "print(\"✅ Interface is ready! Click the link above to start using your assistant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtxKnHklKM8j"
   },
   "source": [
    "Option B: Interface with Llama\n",
    "\n",
    "If you’re using Llama, create a new cell and run this code instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tx9Y8zVvJ-sD"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "\n",
    "def create_llama_interface(model, tokenizer, processor):\n",
    "    \"\"\"Creates an interface for Llama-based assistant\"\"\"\n",
    "    def get_response(question: str) -> str:\n",
    "        \"\"\"Process a question and get Llama's response\"\"\"\n",
    "        try:\n",
    "            # Find relevant document sections\n",
    "            relevant_docs = processor.find_relevant_documents(question)\n",
    "            context = \"\\n\\n\".join(doc.text for doc in relevant_docs)\n",
    "            # Create our prompt\n",
    "            prompt = f\"\"\"Please answer this legal question based on the provided\n",
    "            documents. Be specific and cite relevant sections.\n",
    "            Documents: {context}\n",
    "            Question: {question}\n",
    "            Answer:\"\"\"\n",
    "            # Prepare for Llama\n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=2048,\n",
    "                truncation=True\n",
    "            )\n",
    "            # Generate response\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.7,\n",
    "                do_sample=True\n",
    "            )\n",
    "            # Decode response\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return response.split(\"Answer:\")[-1].strip()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing question: {str(e)}\")\n",
    "            return f\"I encountered an error: {str(e)}\"\n",
    "    # Create the interface\n",
    "    demo = gr.Interface(\n",
    "        fn=get_response,\n",
    "        inputs=[\n",
    "            gr.Textbox(\n",
    "                label=\"Your Legal Question\",\n",
    "                placeholder=\"Ask any question about your legal documents...\",\n",
    "                lines=3\n",
    "            )\n",
    "        ],\n",
    "        outputs=[\n",
    "            gr.Textbox(\n",
    "                label=\"Answer\",\n",
    "                lines=10\n",
    "            )\n",
    "        ],\n",
    "        title=\"Legal Document Assistant (Llama)\",\n",
    "        description=\"\"\"This AI assistant can answer questions about your legal documents.\n",
    "        It uses Llama to provide detailed responses based on your documents.\"\"\"\n",
    "    )\n",
    "    return demo\n",
    "# Create and launch the interface\n",
    "demo = create_llama_interface(model, tokenizer, processor)\n",
    "demo.launch(share=True)  # share=True creates a public link you can share\n",
    "print(\"✅ Interface is ready! Click the link above to start using your assistant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIFTYQD0KWoU"
   },
   "source": [
    "Step 3: Enhanced Interface (Optional)\n",
    "\n",
    "If you want a more sophisticated interface with additional features, create a new cell and run this code (works with either model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "GDuc11vtKXFI",
    "outputId": "d92bbe03-a997-44bb-cf0d-883599285ca7"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-7b47c5f5b820>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-7b47c5f5b820>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    mport gradio as gr\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "\n",
    "def create_advanced_interface(model, processor, model_type=\"gpt-4\"):\n",
    "    \"\"\"Creates an enhanced interface with additional features\"\"\"\n",
    "    def process_query(\n",
    "        question: str,\n",
    "        show_sources: bool,\n",
    "        response_length: str\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"Process a question with additional options\"\"\"\n",
    "        try:\n",
    "            # Find relevant documents\n",
    "            relevant_docs = processor.find_relevant_documents(question)\n",
    "            context = \"\\n\\n\".join(doc.text for doc in relevant_docs)\n",
    "            # Adjust response length\n",
    "            max_tokens = {\n",
    "                \"Brief\": 100,\n",
    "                \"Detailed\": 300,\n",
    "                \"Comprehensive\": 500\n",
    "            }[response_length]\n",
    "            # Get response based on model type\n",
    "            if model_type == \"gpt-4\":\n",
    "                response = model.predict(\n",
    "                    f\"Please provide a {response_length.lower()} answer: \" + context\n",
    "                )\n",
    "            else:  # Llama\n",
    "                inputs = tokenizer(context, return_tensors=\"pt\", truncation=True)\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_tokens,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            # Prepare return value\n",
    "            result = {\"answer\": response}\n",
    "            # Add sources if requested\n",
    "            if show_sources:\n",
    "                sources = \"\\n\\n\".join(\n",
    "                    f\"From {doc.source}:\\n{doc.text[:200]}...\"\n",
    "                    for doc in relevant_docs\n",
    "                )\n",
    "                result[\"sources\"] = sources\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error: {str(e)}\")\n",
    "            return {\"answer\": f\"Error: {str(e)}\", \"sources\": \"\"}\n",
    "    # Create enhanced interface\n",
    "    demo = gr.Interface(\n",
    "        fn=process_query,\n",
    "        inputs=[\n",
    "            gr.Textbox(\n",
    "                label=\"Your Legal Question\",\n",
    "                placeholder=\"Ask any question about your legal documents...\",\n",
    "                lines=3\n",
    "            ),\n",
    "            gr.Checkbox(label=\"Show Source Documents\"),\n",
    "            gr.Radio(\n",
    "                choices=[\"Brief\", \"Detailed\", \"Comprehensive\"],\n",
    "                label=\"Response Length\",\n",
    "                value=\"Detailed\"\n",
    "            )\n",
    "        ],\n",
    "        outputs=[\n",
    "            gr.Textbox(label=\"Answer\", lines=10),\n",
    "            gr.Textbox(label=\"Source Documents\", visible=False) # Shows when sources requested\n",
    "        ],\n",
    "        title=f\"Advanced Legal Assistant ({model_type})\",\n",
    "        description=\"\"\"This enhanced AI assistant can answer questions about your legal documents.\n",
    "        You can customize the response length and choose to see source documents.\"\"\"\n",
    "    )\n",
    "    return demo\n",
    "# Create and launch the enhanced interface\n",
    "demo = create_advanced_interface(model, processor, model_type=\"gpt-4\")  # or \"llama\"\n",
    "demo.launch(share=True)\n",
    "print(\"✅ Enhanced interface is ready! Click the link above to start using your assistant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_AVuU1zKu3o"
   },
   "source": [
    "Interface Features Explained\n",
    "\n",
    "The interfaces we’ve created offer different features:\n",
    "\n",
    "    Basic Interface (Options A and B):\n",
    "\n",
    "    Simple question input\n",
    "    Clear answer output\n",
    "    Model-specific optimizations\n",
    "    Automatic error handling\n",
    "\n",
    "2. Enhanced Interface (Optional):\n",
    "\n",
    "    Adjustable response length\n",
    "    Option to show source documents\n",
    "    More sophisticated error handling\n",
    "    Better context management\n",
    "\n",
    "When using your interface:\n",
    "\n",
    "    The URL provided will work as long as your Colab notebook is running\n",
    "    You can share the link with others (if you used share=True)\n",
    "    The interface will work with your processed documents\n",
    "    Responses might take a few seconds, especially with longer questions\n",
    "\n",
    "Troubleshooting Interface Issues\n",
    "\n",
    "If you encounter problems:\n",
    "\n",
    "    Interface Won’t Load:\n",
    "\n",
    "    Make sure all previous cells ran successfully\n",
    "    Check that Gradio installed correctly\n",
    "    Verify your model connection is working\n",
    "\n",
    "2. Slow Responses:\n",
    "\n",
    "    Try shorter questions first\n",
    "    Reduce the context window size\n",
    "    Use the “Brief” response length option\n",
    "\n",
    "3. Error Messages:\n",
    "\n",
    "    Check your API keys are still valid\n",
    "    Verify your documents were processed correctly\n",
    "    Look for error messages in the Colab output\n",
    "\n",
    "Remember:\n",
    "\n",
    "    Keep your Colab notebook running while using the interface\n",
    "    The public URL changes each time you run the cell\n",
    "    Save your interface URL if you want to share it\n",
    "    Monitor your API usage if using GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AX0yx8HK65R"
   },
   "source": [
    "Optional Advanced Step: Teaching Your Assistant\n",
    "\n",
    "If you’ve been using your legal assistant and want to make it even better at handling your specific legal questions, you can teach it using your own examples. This process has two main parts:\n",
    "\n",
    "    Creating training examples (easier)\n",
    "    Fine-tuning and deploying the model (more advanced)\n",
    "\n",
    "You can do just the first part to prepare your training data, and come back to the second part when you’re ready for the more technical steps.\n",
    "Part 1: Creating Your Training Examples\n",
    "\n",
    "First, let’s create some examples that will help your assistant learn. We’ll use a friendly tool that makes this process easier. Create a new notebook called “Legal_Assistant_Training” and add this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrJ7HnDjKhjg"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "class ExampleCreator:\n",
    "    \"\"\"A friendly tool to help create training examples\"\"\"\n",
    "    def __init__(self):\n",
    "        self.examples = []\n",
    "        self.filename = 'training_data.jsonl'\n",
    "        # Create our input boxes\n",
    "        self.question_box = widgets.Textarea(\n",
    "            description='Question:',\n",
    "            placeholder='Type your legal question here...',\n",
    "            layout={'width': '90%', 'height': '100px'}\n",
    "        )\n",
    "        self.answer_box = widgets.Textarea(\n",
    "            description='Answer:',\n",
    "            placeholder='Type the correct answer here...',\n",
    "            layout={'width': '90%', 'height': '200px'}\n",
    "        )\n",
    "        # Create our buttons\n",
    "        self.save_button = widgets.Button(description='Save Example')\n",
    "        self.save_button.on_click(self.save_example)\n",
    "        self.display_button = widgets.Button(description='Show All Examples')\n",
    "        self.display_button.on_click(self.show_examples)\n",
    "        # Show our tool\n",
    "        display(self.question_box)\n",
    "        display(self.answer_box)\n",
    "        display(self.save_button)\n",
    "        display(self.display_button)\n",
    "    def save_example(self, button):\n",
    "        \"\"\"Save a new example\"\"\"\n",
    "        question = self.question_box.value.strip()\n",
    "        answer = self.answer_box.value.strip()\n",
    "        if not question or not answer:\n",
    "            print(\"❌ Please provide both a question and an answer!\")\n",
    "            return\n",
    "        # Create the example\n",
    "        example = {\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"human\", \"value\": question},\n",
    "                {\"from\": \"gpt\", \"value\": answer}\n",
    "            ]\n",
    "        }\n",
    "        # Save it\n",
    "        with open(self.filename, 'a') as f:\n",
    "            f.write(json.dumps(example) + '\\n')\n",
    "        self.examples.append(example)\n",
    "        # Clear the boxes for next example\n",
    "        self.question_box.value = ''\n",
    "        self.answer_box.value = ''\n",
    "        print(f\"✅ Example saved! You now have {len(self.examples)} examples.\")\n",
    "    def show_examples(self, button):\n",
    "        \"\"\"Show all saved examples\"\"\"\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Your {len(self.examples)} Training Examples:\\n\")\n",
    "        for i, example in enumerate(self.examples, 1):\n",
    "            print(f\"Example {i}:\")\n",
    "            print(f\"Q: {example['conversations'][0]['value']}\")\n",
    "            print(f\"A: {example['conversations'][1]['value']}\\n\")\n",
    "        # Show our tool again\n",
    "        display(self.question_box)\n",
    "        display(self.answer_box)\n",
    "        display(self.save_button)\n",
    "        display(self.display_button)\n",
    "# Create our example creator tool\n",
    "creator = ExampleCreator()\n",
    "print(\"✨ Example Creator is ready! Start adding your training examples above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wh39FNJyLO_Y"
   },
   "source": [
    "Let’s add some starter examples to help you understand the format. Add this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ue_vaT7zLPeL"
   },
   "outputs": [],
   "source": [
    "# Some example legal questions and answers to get you started\n",
    "starter_examples = [\n",
    "    {\n",
    "        \"question\": \"What makes a contract valid?\",\n",
    "        \"answer\": \"A valid contract requires four essential elements: 1) Offer and acceptance, \"\n",
    "                 \"2) Consideration (something of value exchanged), 3) Intention to create \"\n",
    "                 \"legal relations, and 4) Capacity of the parties to contract. The agreement \"\n",
    "                 \"must also be legal and sufficiently certain in its terms.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does force majeure work in contracts?\",\n",
    "        \"answer\": \"Force majeure clauses excuse a party from performing their contractual \"\n",
    "                 \"obligations when extraordinary events beyond their control prevent \"\n",
    "                 \"performance. These events typically include natural disasters, wars, or \"\n",
    "                 \"government actions. The clause must specifically define what constitutes \"\n",
    "                 \"force majeure, and the party claiming it must prove the event's impact.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add these examples using our tool\n",
    "for example in starter_examples:\n",
    "    with open('training_data.jsonl', 'a') as f:\n",
    "        formatted_example = {\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"human\", \"value\": example[\"question\"]},\n",
    "                {\"from\": \"gpt\", \"value\": example[\"answer\"]}\n",
    "            ]\n",
    "        }\n",
    "        f.write(json.dumps(formatted_example) + '\\n')\n",
    "print(\"✅ Added starter examples to your training data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeM2RVMWLbq5"
   },
   "source": [
    "Now you can use the tool to add your own examples. Here are some tips for creating good examples:\n",
    "\n",
    "    Include different types of questions:\n",
    "\n",
    "    What is… (definitions)\n",
    "    How does… (processes)\n",
    "    Why is… (reasoning)\n",
    "    When should… (timing)\n",
    "\n",
    "2. Make sure your answers:\n",
    "\n",
    "    Start with a clear main point\n",
    "    Include specific details\n",
    "    Use proper legal terminology\n",
    "    Stay concise but complete\n",
    "\n",
    "Try to create at least 20–30 examples before moving on to the next part.\n",
    "## Part 2: Fine-Tuning and Deploying Your Model\n",
    "\n",
    "Once you have your training examples ready, you can use them to improve your model. This part is more technical and requires a new Colab notebook with more computational resources.\n",
    "\n",
    "Create a new notebook called “Legal_Assistant_Finetuning” and add these cells:\n",
    "Cell 1: Install Training Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mEyr9o5bLdnm",
    "outputId": "2f0bc354-6c6d-4a3f-b61d-207ed7645601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-4tr4nd6i/unsloth_e4b5dcea890c4a99aa516d0b5d61c0ac\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-4tr4nd6i/unsloth_e4b5dcea890c4a99aa516d0b5d61c0ac\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 5dddf27f3ba94506c48251e907031039eecd40d1\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting unsloth_zoo>=2025.1.2 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading unsloth_zoo-2025.1.3-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.2)\n",
      "Collecting tyro (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tyro-0.9.8-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.49.0.dev0)\n",
      "Collecting datasets>=2.16.0 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\n",
      "Collecting protobuf<4.0.0 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.27.1)\n",
      "Collecting hf_transfer (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting bitsandbytes>=0.43.3 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.5.1+cu121)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n",
      "Collecting xxhash (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.11.11)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.5.1)\n",
      "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2025.1.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.0)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2025.1.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.2.1)\n",
      "Collecting trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth_zoo>=2025.1.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading trl-0.13.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2025.1.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.14.0)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.1.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2025.1.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.1.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.9.4)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.12.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.18.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.1.5)\n",
      "Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2025.1.3-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.8-py3-none-any.whl (113 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.8/113.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading trl-0.13.0-py3-none-any.whl (293 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for unsloth: filename=unsloth-2025.1.5-py3-none-any.whl size=176838 sha256=8a9a4a0b877094cd1e0bff6902d61a5dc22d8c915c89713453c8ad6d76b28758\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5ajf0c9m/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
      "Successfully built unsloth\n",
      "Installing collected packages: xxhash, unsloth, shtab, protobuf, hf_transfer, fsspec, dill, multiprocess, tyro, cut_cross_entropy, bitsandbytes, datasets, trl, unsloth_zoo\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.3\n",
      "    Uninstalling protobuf-5.29.3:\n",
      "      Successfully uninstalled protobuf-5.29.3\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
      "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\n",
      "opentelemetry-proto 1.29.0 requires protobuf<6.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.45.0 cut_cross_entropy-25.1.1 datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 hf_transfer-0.1.9 multiprocess-0.70.16 protobuf-3.20.3 shtab-1.7.1 trl-0.13.0 tyro-0.9.8 unsloth-2025.1.5 unsloth_zoo-2025.1.3 xxhash-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:86: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/bitsandbytes-0.45.0.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
      "  for line in open(toplevel):\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:86: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/cut_cross_entropy-25.1.1.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
      "  for line in open(toplevel):\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:86: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/datasets-3.2.0.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
      "  for line in open(toplevel):\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:86: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/dill-0.3.8.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
      "  for line in open(toplevel):\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:86: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/multiprocess-0.70.16.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
      "  for line in open(toplevel):\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:86: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/protobuf-3.20.3.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
      "  for line in open(toplevel):\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:86: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/shtab-1.7.1.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
      "  for line in open(toplevel):\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:86: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/trl-0.13.0.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
      "  for line in open(toplevel):\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:86: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/unsloth-2025.1.5.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
      "  for line in open(toplevel):\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:86: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/unsloth_zoo-2025.1.3.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
      "  for line in open(toplevel):\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:86: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/xxhash-3.5.0.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
      "  for line in open(toplevel):\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "7456b422629946e8b44407f92247c422",
       "pip_warning": {
        "packages": [
         "google"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xformers<0.0.27\n",
      "  Downloading xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting trl<0.9.0\n",
      "  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
      "Downloading xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl (222.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.7/222.7 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xformers, trl\n",
      "  Attempting uninstall: trl\n",
      "    Found existing installation: trl 0.13.0\n",
      "    Uninstalling trl-0.13.0:\n",
      "      Successfully uninstalled trl-0.13.0\n",
      "Successfully installed trl-0.8.6 xformers-0.0.26.post1\n",
      "✅ Training packages installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:86: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/trl-0.8.6.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
      "  for line in open(toplevel):\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:86: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/xformers-0.0.26.post1.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
      "  for line in open(toplevel):\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# Install specialized training packages\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes datasets\n",
    "\n",
    "print(\"✅ Training packages installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2V4uujULtwQ"
   },
   "source": [
    "Cell 2: Initialize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3u_9sL3_LoJl"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Set up model for training\n",
    "max_seq_length = 2048\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None\n",
    ")\n",
    "# Prepare for fine-tuning\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    use_gradient_checkpointing=True\n",
    ")\n",
    "print(\"✅ Model prepared for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzM9LxMGL5f6"
   },
   "source": [
    "Cell 3: Upload and Train\n",
    "\n",
    "First, upload your training_data.jsonl file that you created in the previous part of this guide, then run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyL9DawkL6Bq"
   },
   "outputs": [],
   "source": [
    "# Load your training data\n",
    "dataset = load_dataset('json', data_files='training_data.jsonl')\n",
    "\n",
    "# Configure training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./legal_assistant_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False\n",
    ")\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"✅ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q3nz9A8MEw8"
   },
   "source": [
    "Cell 4: Save and Share Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBnV5cPcMFiL"
   },
   "outputs": [],
   "source": [
    "# First, log in to Hugging Face\n",
    "from huggingface_hub import login\n",
    "login()  # You'll need to enter your Hugging Face token\n",
    "\n",
    "# Save locally\n",
    "trainer.save_model(\"./FineTunedLegal\")\n",
    "# Push to Hugging Face Hub\n",
    "model_name = \"your-username/LegalAssistant\"  # Change this to your desired name\n",
    "model.push_to_hub(model_name, tokenizer)\n",
    "print(f\"✅ Model saved and pushed to {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Y5hH2m7MOy2"
   },
   "source": [
    "Using Your Improved Model\n",
    "\n",
    "To use your newly trained model, go back to your original legal assistant notebook and update the model loading cell with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkoM4g3EMPOE"
   },
   "outputs": [],
   "source": [
    "# Replace the original model loading code with:\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"your-username/LegalAssistant\",  # Use your model's name\n",
    "    token=\"your-huggingface-token\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"your-username/LegalAssistant\",\n",
    "    token=\"your-huggingface-token\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFcIRLLuMZDd"
   },
   "source": [
    "Your assistant will now use your improved model that’s been trained on your specific examples. It should be better at handling the types of questions you included in your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OuQKr0yjMaJ_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10c9476828b341678bb1189df88cb693": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "390cfd3cee2a47a99f778bde80de17b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "408cdf9700364b3faae550215e9bafb0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a9e7bd7d606425b92e3e06c64363c15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_408cdf9700364b3faae550215e9bafb0",
      "placeholder": "​",
      "style": "IPY_MODEL_390cfd3cee2a47a99f778bde80de17b7",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "5dd4e6f7b4284c64bcc751cc4f5aaf32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4a9e7bd7d606425b92e3e06c64363c15",
       "IPY_MODEL_d235a890782442f0a81f295f656b2117",
       "IPY_MODEL_f71344e37b7746f38871fef74cbcc3c7"
      ],
      "layout": "IPY_MODEL_6f45434fab7040b990777e68317d27e3"
     }
    },
    "6f45434fab7040b990777e68317d27e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89290c0c05aa4192b5726d1f61ca8ba4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9040c75315d748ddba420442f6d4786b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d235a890782442f0a81f295f656b2117": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc7084c581db4380a9de2a2c78432728",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9040c75315d748ddba420442f6d4786b",
      "value": 4
     }
    },
    "f71344e37b7746f38871fef74cbcc3c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10c9476828b341678bb1189df88cb693",
      "placeholder": "​",
      "style": "IPY_MODEL_89290c0c05aa4192b5726d1f61ca8ba4",
      "value": " 4/4 [00:05&lt;00:00,  1.37s/it]"
     }
    },
    "fc7084c581db4380a9de2a2c78432728": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
